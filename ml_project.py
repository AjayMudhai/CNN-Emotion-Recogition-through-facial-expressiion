# -*- coding: utf-8 -*-
"""ML project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1aCh6EhEx-3IO4BrnLTUGxg_WomMZdYPz
"""

import tensorflow as tf
from sklearn import preprocessing

import keras
from keras.models import Sequential
from keras.layers import Conv2D, MaxPooling2D, AveragePooling2D
from keras.layers import Dense, Activation, Dropout, Flatten

from keras.preprocessing import image
from keras.preprocessing.image import ImageDataGenerator

from keras.layers import Dense
from keras.models import model_from_json
import pandas as pd
import numpy as np
from google.colab import files
import matplotlib.pyplot as plt

uploaded  = files.upload()



for fn in uploaded.keys():
      print('User uploaded file "{name}" with length {length} bytes'.format(
      name=fn, length=len(uploaded[fn])))

config= tf.ConfigProto( device_count = {'GPU': 0 , 'CPU':40})
session= tf.Session(config=config) 

keras.backend.set_session(session)

with open("data.csv") as f:
    dataset = f.readlines()
    dataset_array=np.array(dataset)

number_of_classes=7 
batch_size = 256
epochs = 5

# Dividing data into training and test set category as per dataset's third column
x_train =[]
y_train =[]
x_test =[]
y_test = []


for i in range(1,dataset_array.size):
  
        emotion, pixels_list, usage = dataset_array[i].split(",")
          
        val = pixels_list.split(" ")
            
        pixels = np.array(val, 'float32')
        
        emotion = keras.utils.to_categorical(emotion, number_of_classes)
            
    
        if 'Training' in usage:
            y_train.append(emotion)
            x_train.append(pixels)
        elif 'PublicTest' in usage:
            y_test.append(emotion)
            x_test.append(pixels)

x_train = np.array(x_train)
y_train = np.array(y_train)
x_test = np.array(x_test)
y_test = np.array(y_test)
x_train=preprocessing.normalize(x_train)
x_test=preprocessing.normalize(x_test)
print('training samples shape',x_train.shape)
print('testing samples shape', x_test.shape)



x_train = x_train.reshape(x_train.shape[0], 48, 48, 1)
x_train = x_train.astype('float32')

x_test = x_test.reshape(x_test.shape[0], 48, 48, 1)
x_test = x_test.astype('float32')

model = Sequential()

#1st convolution layer
model.add(Conv2D(64, (5, 5), activation='relu', input_shape=(48,48,1)))
model.add(MaxPooling2D(pool_size=(5,5), strides=(2, 2)))

#2nd convolution layer
model.add(Conv2D(64, (3, 3), activation='relu'))
model.add(Conv2D(64, (3, 3), activation='relu'))
model.add(AveragePooling2D(pool_size=(3,3), strides=(2, 2)))

#3rd convolution layer
model.add(Conv2D(128, (3, 3), activation='relu'))
model.add(Conv2D(128, (3, 3), activation='relu'))
model.add(AveragePooling2D(pool_size=(3,3), strides=(2, 2)))

model.add(Flatten())

#fully connected neural networks
model.add(Dense(1024, activation='relu'))
model.add(Dropout(0.2))
model.add(Dense(1024, activation='relu'))
model.add(Dropout(0.2))

model.add(Dense(number_of_classes, activation='softmax'))

gen = ImageDataGenerator()
train_generator = gen.flow(x_train, y_train, batch_size=batch_size)

#------------------------------

model.compile(loss='categorical_crossentropy'
    , optimizer=keras.optimizers.Adam()
    , metrics=['accuracy']
)

fit = True

if fit == True:
	
	model.fit_generator(train_generator, steps_per_epoch=batch_size, epochs=epochs) 
else:
	model.load_weights('mod.h5')

#overall evaluation
score = model.evaluate(x_test, y_test)
print('Test loss:', score[0])
print('Test accuracy:', 100*score[1])

# serialize model to JSON
model_json = model.to_json()
with open("model.json", "w") as json_file:
    json_file.write(model_json)
# serialize weights to HDF5
model.save_weights("model.h5")

!ls

files.download("model.json")

files.download("model.h5")